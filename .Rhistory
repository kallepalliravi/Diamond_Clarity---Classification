rf.error = confusionMatrix(rf.in.pred,valid.rf$clarity.bin )
error.in[n] = 1-sum(diag(rf.error))/sum(rf.error)
}
}
n = dim(train.rf)[1]
k = 10
groups = c(rep(1:k,floor(n/k)), 1:(n-floor(n/k)*k))
set.seed(123)
cvgroups = sample(groups,n)
error.in = rep(NA,n)
levels = c(1,2,3,4,5,6,7,8,9)
for(i in 1:k){
groupi = (cvgroups==i)
for(n in 1:9){
rf.in = randomForest(clarity.bin ~., data=groupi, ntree=100, mtry = levels[[n]])
rf.in.pred = predict(rf.in, !groupi)
rf.error = confusionMatrix(rf.in.pred,valid.rf$clarity.bin )
error.in[i,n] = 1-sum(diag(rf.error))/sum(rf.error)
}
error.in
library(caret)
ctrl = trainControl(method="CV", number=10, savePredictions = TRUE, classProbs = TRUE)
mtry.cv = expand.grid(mtry = c(2,3,4,5,6,7,8,9))
ctrl = trainControl(method="CV", number=10, savePredictions = TRUE, classProbs = TRUE)
mtry.cv = expand.grid(mtry = c(2,3,4,5,6,7,8,9))
rf.model.cv = train(clarity.bin ~. , data= train.rf,
method = "rf",
trControl = ctrl,
tuneGrid = mtry.cv
)
rf.model.cv
diamonds.rf = read.csv("diamonds.csv")
head(diamonds.rf)
# first col is just row number so delete that col
diamonds.rf = diamonds.rf[,-1]
dim(diamonds.rf)
## check the structure of data
str(diamonds.rf)
## check for any NA's in data
anyNA(diamonds.rf)
## check the summary of data
summary(diamonds.rf)
summary(diamonds.rf)
library(psych)
corr.test(diamonds.rf[-c(2:4)])
diamonds.rf$clarity.bin = rep(NA, length(diamonds.rf$clarity))
diamonds.rf$clarity.bin[which(diamonds.rf$clarity=="VS1" | diamonds.rf$clarity=="VS2" )] = "VS"
diamonds.rf$clarity.bin[which(diamonds.rf$clarity=="VVS1" | diamonds.rf$clarity=="VVS2" )] = "VVS"
diamonds.rf$clarity.bin[which(diamonds.rf$clarity=="SI1" | diamonds.rf$clarity=="SI2" )] = "SI"
diamonds.rf$clarity.bin[which(diamonds.rf$clarity=="IF" | diamonds.rf$clarity=="IF2" )] = "IF"
diamonds.rf$clarity.bin[which(diamonds.rf$clarity=="I1" | diamonds.rf$clarity=="I1" )] = "I1"
anyNA(diamonds)
attach(diamonds.rf)
diamonds.rf.df = data.frame(clarity.bin, cut,color,carat,depth,table,price,x,y,z)
head(diamonds.rf.df)
set.seed(123)
n=dim(diamonds.rf.df)[1]
s.rf = sample(n, 0.66*n)
train.rf = diamonds.rf.df[s.rf,]
valid.rf = diamonds.rf.df[-s.rf,]
dim(train.rf)
dim(valid.rf)
head(train.rf)
plot(diamonds.rf.df$clarity.bin)
rf.model.cv
?randomForest
library(randomForest)
?randomForest
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE,
)
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE
)
rf.tune
rf.tune.pred = predict(rf.tune, valid.rf, type="prob")
rf.tune.pred
write.csv(rf.tune.pred, "prob.csv")
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE,
cutoff = c(0.3,0.3,0.5,0.5,0.5)
)
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE,
cutoff = c(0.3,0.3,0.5,0.5,0.5)
)
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE,
cutoff = c(0.3,1-0.3,1-0.3,1-0.3,1-0.3)
)
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE,
cutoff = c(0.5,0.5,0.5,0.5,0.5)
)
library(caret)
ctrl = trainControl(method="CV", number=10, savePredictions = TRUE, classProbs = TRUE)
mtry.cv = expand.grid(mtry = c(2,3,4,5,6,7,8,9))
rf.model.cv = train(clarity.bin ~. , data= train.rf,
method = "rf",
trControl = ctrl,
tuneGrid = mtry.cv,
metric ="ROC"
)
diamonds.rf.tune = tuneRF(train.rf[,-1], train.rf[,1],
ntreeTry = 100,
stepFactor = 0.5,
mtryStart = 3,
improve = 0.05,
trace = TRUE,
plot = TRUE)
diamonds.rf.tune = tuneRF(train.rf[,-1], train.rf[,1],
ntreeTry = 100,
stepFactor = 0.5,
mtryStart = 3,
improve = 0.05,
trace = TRUE,
plot = TRUE)
set.seed(123)
rf.tune = randomForest(clarity.bin ~.,data=train.rf,
ntree = 100,
mtry=6,
importance = TRUE)
rf.tune
rf.tune.pred = predict(rf.tune, valid.rf)
confusionMatrix(rf.tune.pred,valid.rf$clarity.bin )
varImpPlot(rf.tune)
plot(rf.tune.pred)
ctrl = trainControl(method="CV", number=10, savePredictions = TRUE)
mtry.cv = expand.grid(mtry = c(2,3,4,5,6,7,8,9))
rf.model.cv = train(clarity.bin ~. , data= train.rf,
method = "rf",
trControl = ctrl,
tuneGrid = mtry.cv
)
rf.model.cv
plot(rf.model.cv)
plot(rf.model.cv, xlab="mtry")
setwd("C:/Users/Ravi/Desktop/UW-MS/DS740-DataMining/Course Material/Final Project")
diamonds.da = read.csv("diamonds.csv")
diamonds.da = read.csv("diamonds.csv")
diamonds.da = diamonds.da[,-1]
dim(diamonds.da)
## check the structure of data
str(diamonds.da)
## check for any NA's in data
anyNA(diamonds.da)
## check the summary of data
summary(diamonds.da)
summary(diamonds.da)
diamonds.da$clarity.bin = rep(NA, length(diamonds.da$clarity))
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VS1" | diamonds.da$clarity=="VS2" )] = "VS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VVS1" | diamonds.da$clarity=="VVS2" )] = "VVS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="SI1" | diamonds.da$clarity=="SI2" )] = "SI"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="IF" | diamonds.da$clarity=="IF2" )] = "IF"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="I1" | diamonds.da$clarity=="I1" )] = "I1"
anyNA(diamonds.da)
hist(diamonds.da$carat)
hist(diamonds$depth)
hist(diamonds$table)
hist(diamonds.df$price)
hist(diamonds.da$depth)
hist(diamonds.da$table)
hist(diamonds.da$price)
hist(diamonds.da$x)
hist(diamonds.da$y)
hist(diamonds.da$z)
diamonds.da$carat.log = log(diamonds.da$carat)
hist(diamonds.da$carat.log)
diamonds.da$price.log = log(diamonds.da$price)
hist(diamonds.da$price.log)
diamonds.da$y.log = log(diamonds.da$y)
hist(diamonds.da$y.log)
diamonds.da$z.log = log(diamonds.da$z)
hist(diamonds.da$z.log)
attach(diamonds.da)
diamonds.da.df = data.frame(clarity.bin, cut,color, carat.log, depth, table, price.log, x, y.log, z.log)
head(diamonds.da.df)
diamonds.da$carat.log.scale = scale(diamonds$carat.log,center = TRUE, scale = TRUE)
diamonds.da$carat.log.scale = scale(diamonds$carat.log,center = TRUE, scale = TRUE)
diamonds.da$carat.log.scale = scale(diamonds.da$carat.log,center = TRUE, scale = TRUE)
diamonds.da$depth.scale = scale(diamonds.da$depth,center = TRUE, scale = TRUE)
diamonds.da$table.scale = scale(diamonds.da$table,center = TRUE, scale = TRUE)
diamonds.da$price.log.scale = scale(diamonds.da$price.log,center = TRUE, scale = TRUE)
diamonds.da$x.scale = scale(diamonds.da$x,center = TRUE, scale = TRUE)
diamonds.da$y.log.scale = scale(diamonds.da$y.log,center = TRUE, scale = TRUE)
diamonds.da$z.log.scale = scale(diamonds.da$z.log,center = TRUE, scale = TRUE)
head(diamonds.da)
diamonds.da$y.log.scale = scale(diamonds.da$y.log,center = TRUE, scale = TRUE)
head(diamonds.da)
anyNA(diamonds.da)
head(diamonds.da)
diamonds.da
anyNA(diamonds.da)
head(diamonds.da)
diamonds.da$y.log.scale = scale(diamonds.da$y.log,center = TRUE, scale = TRUE)
head(diamonds.da)
diamonds.da$y.log.scale = scale(diamonds.da$y,center = TRUE, scale = TRUE)
head(diamonds.da)
hist(diamonds.da$y.log.scale)
hist(log(diamonds.da$y.log.scale))
detach(diamonds.da)
diamonds.da = read.csv("diamonds.csv")
diamonds.da = diamonds.da[,-1]
dim(diamonds.da)
## check the structure of data
str(diamonds.da)
## check for any NA's in data
anyNA(diamonds.da)
## check the summary of data
summary(diamonds.da)
summary(diamonds.da)
diamonds.da$clarity.bin = rep(NA, length(diamonds.da$clarity))
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VS1" | diamonds.da$clarity=="VS2" )] = "VS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VVS1" | diamonds.da$clarity=="VVS2" )] = "VVS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="SI1" | diamonds.da$clarity=="SI2" )] = "SI"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="IF" | diamonds.da$clarity=="IF2" )] = "IF"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="I1" | diamonds.da$clarity=="I1" )] = "I1"
anyNA(diamonds.da)
hist(diamonds.da$carat) ## not normal and right skewed, requires transformation
hist(diamonds.da$depth) ## aprox normal
hist(diamonds.da$table) ## approx normal
hist(diamonds.da$price) ## not normal and right skewed, requires transformation
hist(diamonds.da$x) ## approx normal
hist(diamonds.da$y)  ##not normal and right skewed, requires transformation
hist(diamonds.da$z) ##not normal and right skewed, requires transformation
hist(diamonds.da$z) ##not normal and right skewed, requires transformation
diamonds.da$carat.log = log(diamonds.da$carat)
hist(diamonds.da$carat.log) ## aprox normal
diamonds.da$price.log = log(diamonds.da$price)
hist(diamonds.da$price.log) ## approx normal
diamonds.da$y.log = log(diamonds.da$y)
hist(diamonds.da$y.log)  ## approx normal
diamonds.da$z.log = log(diamonds.da$z)
hist(diamonds.da$z.log)  ## approx normal
hist(diamonds.da$carat) ## not normal and right skewed, requires transformation
hist(diamonds.da$depth) ## aprox normal
hist(diamonds.da$table) ## approx normal
hist(diamonds.da$price) ## not normal and right skewed, requires transformation
hist(diamonds.da$x) ## approx normal
hist(diamonds.da$y)  ##not normal and right skewed, requires transformation
hist(diamonds.da$z) ##not normal and right skewed, requires transformation
attach(diamonds.da)
diamonds.da.df = data.frame(clarity.bin, cut,color, carat.log, depth, table, price.log, x, y.log, z.log)
head(diamonds.da.df)
library(MASS)
ldafit1 = lda(clarity.bin ~ ., data = diamonds.da.df)
anyNA(diamonds.da.df)
head(diamonds.da.df)
ldafit1 = lda(clarity.bin ~ price.log, data = diamonds.da.df)
ldafit1
ldafit1 = lda(clarity.bin ~ price.log+cut, data = diamonds.da.df)
ldafit1
ldafit1 = lda(clarity.bin ~ price.log+cut+color, data = diamonds.da.df)
ldafit1
ldafit1 = lda(clarity.bin ~ price.log+cut+color+carat.log, data = diamonds.da.df)
ldafit1
ldafit1 = lda(clarity.bin ~ price.log+cut+color+carat.log+y.log, data = diamonds.da.df)
anyNA(diamonds.da.df$y.log)
write.csv(diamonds.da.df, "da.df.csv")
detach(diamonds.da)
diamonds.da = read.csv("diamonds.csv")
diamonds.da = diamonds.da[,-1]
dim(diamonds.da)
## check the structure of data
str(diamonds.da)
## check for any NA's in data
anyNA(diamonds.da)
## check the summary of data
summary(diamonds.da)
summary(diamonds.da)
diamonds.da$clarity.bin = rep(NA, length(diamonds.da$clarity))
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VS1" | diamonds.da$clarity=="VS2" )] = "VS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="VVS1" | diamonds.da$clarity=="VVS2" )] = "VVS"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="SI1" | diamonds.da$clarity=="SI2" )] = "SI"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="IF" | diamonds.da$clarity=="IF2" )] = "IF"
diamonds.da$clarity.bin[which(diamonds.da$clarity=="I1" | diamonds.da$clarity=="I1" )] = "I1"
anyNA(diamonds.da)
hist(diamonds.da$carat) ## not normal and right skewed, requires transformation
hist(diamonds.da$depth)
hist(diamonds.da$table)
hist(diamonds.da$price) ## not normal and right skewed, requires transformation
hist(diamonds.da$x)
hist(diamonds.da$y)
hist(diamonds.da$z)
diamonds.da$carat.log = log(diamonds.da$carat)
hist(diamonds.da$carat.log) ## aprox normal
diamonds.da$price.log = log(diamonds.da$price)
hist(diamonds.da$price.log) ## approx normal
attach(diamonds.da)
diamonds.da.df = data.frame(clarity.bin, cut,color, carat.log, depth, table, price.log, x, y, z)
head(diamonds.da.df)
head(diamonds.da.df)
ldafit1 = lda(clarity.bin ~ ., data = diamonds.da.df)
ldafit1
fittedclass1 = predict(ldafit1,data=diamonds.da.df)$class
table(Auto$domestic,fittedclass1)
fittedclass1 = predict(ldafit1,data=diamonds.da.df)$class
table(diamonds.da.df$clarity.bin,fittedclass1)
set.seed(123)
n=dim(diamonds.da.df)[1]
s.da = sample(n, 0.66*n)
train.da = diamonds.da.df[s.rf,]
valid.da = diamonds.da.df[-s.rf,]
dim(train.da)
dim(valid.da)
head(train.da)
dim(train.da)
dim(valid.da)
nmodels=9
model1 = (clarity.bin ~ price.log)
model2 = (clarity.bin ~ price.log+color)
model3 = (clarity.bin ~ price.log+color+depth)
model4 = (clarity.bin ~ price.log+color+depth+carat.log)
model5 = (clarity.bin ~ price.log+color+depth+carat.log+y)
model6 = (clarity.bin ~ price.log+color+depth+carat.log+y+cut)
model7 = (clarity.bin ~ price.log+color+depth+carat.log+y+cut+table)
model8 = (clarity.bin ~ price.log+color+depth+carat.log+y+cut+table+z)
model9 = (clarity.bin ~ price.log+color+depth+carat.log+y+cut+table+z+x)
allmodels = list(model1,model2,model3,model4,model5,model6,model7,model8,model9)
n = dim(train.da)[1]
k =10
groups = c(rep(1:m,floor(n/k))
set.seed(123)
n = dim(train.da)[1]
k =10
groups = c(rep(1:m,floor(n/k))
n = dim(train.da)[1]
n = dim(train.da)[1]
k =10
groups = c(rep(1:m,floor(n/k)))
n = dim(train.da)[1]
k =10
groups = c(rep(1:k,floor(n/k)))
n = dim(train.da)[1]
k =10
groups = c(rep(1:k,floor(n/k)))
set.seed(123)
cvgroups = sample(groups,n)
allpredictedCV.lda = matrix(rep(NA,n*nmodels),ncol=nmodels)
for(i in 1:k){
groupi = (cvgroups == i)
for (m in 1:nmodels)  {
ldafit = lda(formula = allmodel[[m]],data=train.da, subset= !groupi )
newdata = data.frame(train.da[groupi,])
allpredictedCV.lda[groupi,m] = as.character(predict(ldafit,newdata)$class)
}
}
n = dim(train.da)[1]
k =10
groups = c(rep(1:k,floor(n/k)))
set.seed(123)
cvgroups = sample(groups,n)
allpredictedCV.lda = matrix(rep(NA,n*nmodels),ncol=nmodels)
for(i in 1:k){
groupi = (cvgroups == i)
for (m in 1:nmodels)  {
ldafit = lda(formula = allmodels[[m]],data=train.da, subset= !groupi )
newdata = data.frame(train.da[groupi,])
allpredictedCV.lda[groupi,m] = as.character(predict(ldafit,newdata)$class)
}
}
head(allpredictedCV.lda)
allmodelCV.lda = rep(NA,nmodels)
for (m in 1:nmodels) { allmodelCV.lda[m] = sum(allpredictedCV.lda!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.lda,col="red",pch=20)
allmodelCV.lda
allpredictedCV.lda
table(train.da$clarity.bin,allpredictedCV.lda)
table(train.da$clarity.bin,allpredictedCV.lda[1])
for (m in 1:nmodels) { allmodelCV.lda[m] = sum(allpredictedCV.lda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.lda,col="red",pch=20)
allpredictedCV.lda
allmodelCV.lda
plot(1:nmodels,allmodelCV.lda,col="red",pch=20)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log)
lda.tune
lda.fit = predict(lda.tune,data=valid.da)$class
table(Actual = valid.da$clarity.bin,Predicted=lda.fit)
lda.fit = predict(lda.tune,data=valid.da)$class
table(valid.da$clarity.bin,lda.fit)
lda.fit
dim(train.da)
dim(valid.da)
head(valid.da)
dim(valid.da)
lda.fit = predict(lda.tune,data=valid.da)$class
lda.fit = predict(lda.tune,data=valid.da)$class
table(valid.da$clarity.bin,lda.fit)
lda.fit
summary(lda.fit)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log, data=train.da)
lda.tune
lda.fit = predict(lda.tune,data=valid.da)$class
table(valid.da$clarity.bin,lda.fit)
summary(lda.fit)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log, data=valid.da)
lda.tune
lda.fit = predict(lda.tune,data=valid.da)$class
table(valid.da$clarity.bin,lda.fit)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log, data=train.da)
lda.tune
table(train.da$clarity.bin,lda.tune)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log, data=train.da)
lda.tune
table(train.da$clarity.bin,lda.tune)
lda.tune = lda(clarity.bin ~ price.log+color+depth+carat.log, data=train.da)
lda.tune
table(valid.da$clarity.bin,lda.fit)
n = dim(train.da)[1]
k =10
groups = c(rep(1:k,floor(n/k)))
set.seed(123)
cvgroups = sample(groups,n)
allpredictedCV.lda = matrix(rep(NA,n*nmodels),ncol=nmodels)
allpredictedCV.qda = matrix(rep(NA,n*nmodels),ncol=nmodels)
for(i in 1:k){
groupi = (cvgroups == i)
for (m in 1:nmodels)  {
ldafit = lda(formula = allmodels[[m]],data=train.da, subset= !groupi )
newdata.lda = data.frame(train.da[groupi,])
allpredictedCV.lda[groupi,m] = as.character(predict(ldafit,newdata.lda)$class)
}
for (n in 1:nmodels)  {
qdafit = qda(formula = allmodels[[m]],data=train.da, subset= !groupi )
newdata.qda = data.frame(train.da[groupi,])
allpredictedCV.qda[groupi,n] = as.character(predict(qdafit,newdata.qda)$class)
}
}
allmodelCV.lda = rep(NA,nmodels)
for (m in 1:nmodels) { allmodelCV.lda[m] = sum(allpredictedCV.lda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.lda,col="red",pch=20)
allmodelCV.qda = rep(NA,nmodels)
for (n in 1:nmodels) { allmodelCV.qda[m] = sum(allpredictedCV.qda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.qda,col="red",pch=20)
allmodelCV.qda = rep(NA,nmodels)
for (n in 1:nmodels) { allmodelCV.qda[n] = sum(allpredictedCV.qda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.qda,col="red",pch=20)
allmodelCV.lda = rep(NA,nmodels)
for (m in 1:nmodels) { allmodelCV.lda[m] = sum(allpredictedCV.lda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.lda,col="red",pch=20)
allmodelCV.qda = rep(NA,nmodels)
for (n in 1:nmodels) { allmodelCV.qda[n] = sum(allpredictedCV.qda[,m]!=train.da$clarity.bin)/n}
plot(1:nmodels,allmodelCV.qda,col="red",pch=20)
qda.tune = lda(clarity.bin ~ ., data=train.da)
qda.tune
qda.fit = predict(qda.tune,data=valid.da)$class
table(valid.da$clarity.bin,qda.fit)
qda.fit = predict(qda.tune,data=valid.da)
table(valid.da$clarity.bin,qda.fit)
qda.fit$class
table(train.da$clarity.bin,qda.tune)
table(train.da$clarity.bin,qda.fit)
lda.fit = predict(lda.tune,data=valid.da)$class
table(valid.da$clarity.bin,lda.fit)
qda.fit = predict(qda.tune,data=valid.da)
table(qda.fit)
length(qda.fit)
qda.fit
table(qda.fit)$class
table(qda.fit$class)
table(qda.fit$class, valid.da$clarity.bin)
length(qda.fit$class)
length(valid.da)
dim(valid.da)
dim(valid.da$clarity.bin)
valid.da$clarity.bin
qda.fit$class
qda.fit = predict(qda.tune,data=valid.da)$class
qda.fit
table(qda.fit, valid.da$clarity.bin)
qda.fit = predict(qda.tune,valid.da)$class
table(qda.fit, valid.da$clarity.bin)
table(valid.da$clarity.bin, qda.fit)
qda.tab = table(valid.da$clarity.bin, qda.fit)
qda.tab
sum(diag(qda.tab))/sum(qda.tab)
lda.fit = predict(lda.tune,valid.da)$class
lda.tab = table(valid.da$clarity.bin, lda.fit)
lda.tab
sum(diag(lda.tab))/sum(lda.tab)
allpredictedCV.lda
allmodelCV.lda
allpredictedCV.lda[,1]
sum(allpredictedCV.lda[,1])
sum(allpredictedCV.lda[,1]!=train.da$clarity.bin)
allpredictedCV.lda[,1]!=train.da$clarity.bin
table(allpredictedCV.lda[,1]!=train.da$clarity.bin)
sum(allpredictedCV.lda[,1]!=train.da$clarity.bin)
sum(allpredictedCV.lda[,1]!=train.da$clarity.bin)/n
rf.tab
rf.tab = confusionMatrix(rf.tune.pred,valid.rf$clarity.bin )
rf.tab
rf.tab
table(rf.tune.pred, valid.rf$clarity.bin)
